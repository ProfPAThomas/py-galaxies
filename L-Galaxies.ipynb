{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python implementation of L-Galaxies\n",
    "\n",
    "This is a playground to test out the possibility of using `python` as an interface into L-Galaxies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import astropy.constants as c\n",
    "import astropy.units as u\n",
    "import gc\n",
    "import h5py\n",
    "h5py.enable_ipython_completer()\n",
    "import numpy as np\n",
    "import yaml\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook')\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Development limiter\n",
    "nHaloMax=np.inf\n",
    "\n",
    "# Debug/testing switch\n",
    "debugFlag=True\n",
    "\n",
    "# Verbosity\n",
    "verbosity=1 # 0 - Major program steps only; 1/2 - Major/minor Counters; 3 - Debugging diags.\n",
    "\n",
    "# Script parameters\n",
    "file_parameters='input/input.yml'\n",
    "displayParameters=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputFiles          : {'graphFile': 'MergerGraphs/FullGraphs_subsample.hdf5'}\n",
      "outputFiles         : {'haloFile': 'output/SMT13HaloOutput.hdf5', 'galaxyFile': 'output/SMT13GalaxyOutput.hdf5'}\n",
      "cosmology           : {'OmegaM': {'Description': 'Matter density parameter', 'Value': 0.3, 'Units': 'None'}, 'fBaryon': {'Description': 'Baryon fraction', 'Value': 0.155, 'Units': 'None'}}\n",
      "modelSwitches       : {'HOD': {'Description': 'Halo occupation description for stars (as primitive test)', 'Value': True}}\n",
      "performance         : {'io_nRec': {'Description': 'Size of HDF5 io buffer (number of records)', 'Value': 1000}}\n"
     ]
    }
   ],
   "source": [
    "# Read in parameters, graph file, etc.\n",
    "\n",
    "parameters=yaml.load(open(file_parameters),Loader=yaml.FullLoader)\n",
    "if displayParameters:\n",
    "    for item in parameters:\n",
    "        print(\"{:20s}: {}\".format(item,parameters[item]))\n",
    "\n",
    "fBaryon=parameters['cosmology']['fBaryon']['Value']\n",
    "io_nRec=parameters['performance']['io_nRec']['Value']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data structure for halos\n",
    "\n",
    "This is an interesting problem.  We have many requirements:\n",
    "- Must be fast – does this remove the possibility of using objects?\n",
    "- Must be flexible enough to respond to parameter choices (ideally at run time).\n",
    "- Must allow for variable-length arrays – I think that each halo will individually need to track what fraction of material it inherits from each progenitor.\n",
    "\n",
    "I think that the object-oriented way of doing it, as below, can easily adapt to run-time choices because it does not use long arrays.\n",
    "\n",
    "I have initially coded it using:\n",
    "- a list of graphs;\n",
    "- each graph is a dictionary of snapshots;\n",
    "- each snapshot is a dictionary of halos;\n",
    "- each halo is an instance of the `haloProperties` class.  \n",
    "\n",
    "There is a numpy.object dtype that would allow one to make that numpy arrays but I don't know if it offers any performance advantages or disadvantages.  The numpy objects can have arbitrary data added to them, but again I don't know if this flexibility means that they will be very slow (due to having to continually shift things around in memory).\n",
    "\n",
    "This is all very far from the current method that we have in L-Galaxies of defining a galaxy structure at compile time.\n",
    "\n",
    "## Data structure in graph HDF5 files\n",
    "\n",
    "Each graph is a group in the root `['/']` with labels `graphID` that appear to be non-consecutive: `group size=918`; `max(graphID)=1000`.  However, this could be because this is a subset of all graphs – in general may be working with subsets, so don't assume consecutive.\n",
    "\n",
    "### Snapshots\n",
    "\n",
    "Each snapshot is a group in `['/<graphID>']` with labels `snapID` running over all snapshots.\n",
    "\n",
    "### Halos\n",
    "\n",
    "Each halo is a group in `['/<graphID>/<snapID>']` with labels `haloID` that appear to run consecutively from 0, with `haloID` increasing with `snapID`.<br>\n",
    "<b>Note: they are in increasing string order, not numerical order.  This is a feature that needs fixing or it will break the code at some point.</b><br>\n",
    "<b>Note further: can just recode halo lists as dictionaries with `haloID` as lookup key.  That is much more robust and O(1) lookup.</b>\n",
    "\n",
    "Halos have attributes:\n",
    "* `catalogID` – ID in the original halo catalogue\n",
    "* `concentration` – ?\n",
    "* `halo_mass` – mass in Msun\n",
    "* `halo_nPart` – number of particles\n",
    "\n",
    "and datasets:\n",
    "* `centre_of_mass` – `np.float64[3]` array of centroid (?) in cMpc (?)\n",
    "* `desc_haloIDs` – `np.int64[<variable>]` array of `haloID`s of decendants\n",
    "* `desc_mass_contribution` – `np.int64[<variable>]` array of number of particles in common with each descendant (? – may contain non-associated particles)\n",
    "* `halo_velocity` – `np.float64[3]` array of mean velocity in km/s (?)\n",
    "* `prog_haloIDs` – `np.int64[<variable>]` array of `progID`s of decendants\n",
    "* `prog_mass_contribution` – `np.int64[<variable>]` array of number of particles in  \n",
    "common with each progenitor (? – may contain non-associated particles)\n",
    "\n",
    "Presumably we can add more properties, as desired.\n",
    "\n",
    "Halos also have subhalo groups contained within them.\n",
    "\n",
    "### Subhalos\n",
    "\n",
    "Each subhalo is a group in `['/<graphID>/<snapID>/<haloID>']` with labels `subhaloID` that run consecutively from 1 within each halo.  \\[It is not obvious that this is optimal – we may want to change.\\]<br>\n",
    "<b>Note: they are in increasing string order, not numerical order.  This is a feature that needs fixing or it will break the code at some point.</b><br>\n",
    "<b>Note further: can just recode halo lists as dictionaries with `haloID` as lookup key.  That is much more robust and O(1) lookup.</b>\n",
    "\n",
    "Subalos have attributes:\n",
    "* `subhalo_nPart` – number of particles\n",
    "\n",
    "and datasets:\n",
    "* `Subhalo_Part_IDs` – IDs of particles in subhalo\n",
    "* `Subhalo_Pos` – postions of particles in subhalo\n",
    "* `Subhalo_Vel` – velocities of particles in subhalo\n",
    "* `subhalo_mean_pos` – mean position of subhalo\n",
    "* `subhalo_mean_vel` – mean velocity of subhalo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This haloProperties class is just a container for all the halo properties.\n",
    "# It is not expected that it should have any sophisticated methods.\n",
    "# The constructer merely provides the unique labels for each halo;\n",
    "# other properties may then be added.\n",
    "# If this dynamic variable allocation is too slow, we could presumably find a way\n",
    "# to declare all the variables that we will need at the time of construction.\n",
    "class haloProperties:\n",
    "    # Constructor.\n",
    "    def __init__(self,graphID,snapID,haloID):\n",
    "        # These are (HDF5) strings\n",
    "        self.graphID=graphID\n",
    "        self.snapID=snapID\n",
    "        self.haloID=haloID\n",
    "        # Other halo properties.  Is it best to initialise to particular type or to None?\n",
    "        self.done=False\n",
    "        self.catalogID=-1\n",
    "        self.mass=0.\n",
    "        self.massBaryon=0.\n",
    "        self.mass_fromProgenitors=0.\n",
    "        self.massBaryon_fromProgenitors=0.\n",
    "        if parameters['modelSwitches']['HOD']: \n",
    "            self.massStars=0.\n",
    "            self.massStars_fromProgenitors=0.\n",
    "        # Not clear whether it is better to store information here about\n",
    "        # progenitors and descendants, which might result in variable-length\n",
    "        # data structure, or to look up from the HDF5 data file as required,\n",
    "        # which might require multiple access.\n",
    "        # If we want to specify the data size in advance, then we need to\n",
    "        # decide upon a maximum number of descendants to keep.  (I suspect\n",
    "        # that keeping as few as 3 descendants may be enough.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "Most of the work to be done in external routines, probably to be coded in C for efficiency.\n",
    "\n",
    "Here we just include the high-level driver routines.\n",
    "\n",
    "### I/O routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open graph data file\n",
    "def openGraphInput():\n",
    "    return h5py.File(parameters['inputFiles']['graphFile'],'r')\n",
    "\n",
    "# Close graph data file\n",
    "def closeGraphInput(graphInputFile):\n",
    "    graphInputFile.close()\n",
    "\n",
    "# Open galaxy output file\n",
    "def openGalaxyOutput(galaxyOutputFile):\n",
    "    galaxyOutputFile=h5py.File(parameters['outputFiles']['galaxyFile'],'w')\n",
    "\n",
    "# Close galaxy output file\n",
    "def closeGalaxyOutput():\n",
    "    galaxyOutputFile.close()\n",
    "\n",
    "# Open halo output file and create iobuffer and empty output dataset\n",
    "def openHaloOutput():\n",
    "    haloOutputFile=h5py.File(parameters['outputFiles']['haloFile'],'w')\n",
    "    haloOutputData=np.empty(io_nRec,dtype=dtype_halo)\n",
    "    haloOutputDataset=haloOutputFile.create_dataset('Halos',(0,),maxshape=(None,),dtype=dtype_halo,compression='gzip')\n",
    "    haloOutput_iRec=0\n",
    "    return haloOutputFile,haloOutputData,haloOutputDataset,haloOutput_iRec\n",
    "    \n",
    "# Close galaxy output file\n",
    "def closeHaloOutput(haloOutputFile,haloOutputData,haloOutputDataset,haloOutput_iRec):\n",
    "    # Write out halos remaining in output buffer\n",
    "    if haloOutput_iRec>0: haloOutput_iRec=flushOutput(haloOutput_iRec,haloOutputData,haloOutputDataset)\n",
    "    haloOutputFile.close()\n",
    "    return haloOutput_iRec\n",
    "\n",
    "# Output desired halo properties\n",
    "# For now just dump into a single dataset.  Will worry about whether we want a different format later\n",
    "# First need to define the dtype for the numpy structured array output.  Use 32 bit to save space.\n",
    "dtype_halo=np.dtype([\n",
    "    ('graphID',np.int32),\n",
    "    ('snapID',np.int32),\n",
    "    ('haloID',np.int32),\n",
    "    ('catalogID',np.int64),\n",
    "    ('mass',np.float32),\n",
    "    ('massBaryon',np.float32),\n",
    "    ('mass_fromProgenitors',np.float32)\n",
    "])\n",
    "def outputHalos(halos,haloOutputData,haloOutputDataset,haloOutput_iRec):\n",
    "    # Construct structured numpy array with desired data in\n",
    "    for halo in halos:\n",
    "        haloOutputData[haloOutput_iRec]['graphID']=halo.graphID\n",
    "        haloOutputData[haloOutput_iRec]['snapID']=halo.snapID\n",
    "        haloOutputData[haloOutput_iRec]['haloID']=halo.haloID\n",
    "        haloOutputData[haloOutput_iRec]['catalogID']=halo.catalogID\n",
    "        haloOutputData[haloOutput_iRec]['mass']=halo.mass\n",
    "        haloOutputData[haloOutput_iRec]['massBaryon']=halo.massBaryon\n",
    "        haloOutputData[haloOutput_iRec]['mass_fromProgenitors']=halo.mass_fromProgenitors\n",
    "        haloOutput_iRec+=1\n",
    "        if haloOutput_iRec==io_nRec: haloOutput_iRec=flushOutput(haloOutput_iRec,haloOutputData,haloOutputDataset)\n",
    "    return haloOutput_iRec\n",
    "    \n",
    "# Writes numpy output array to HDF5 dataset\n",
    "def flushOutput(nRec,outputData,outputDataset):\n",
    "    outputDataset.resize((outputDataset.shape[0]+nRec,))\n",
    "    outputDataset[-nRec:]=outputData[:nRec]\n",
    "    nRec=0\n",
    "    return nRec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Halo processing routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing halos \n",
    "def processHalo(halo):\n",
    "    if verbosity>=2: print('Processing halo ',halo.haloID)\n",
    "    if halo.done==True: \n",
    "        print('Warning: processHalo: halo ',str(halo),' already processed.')\n",
    "        return\n",
    "    readProperties(halo)\n",
    "    calcMassToDesc(halo)\n",
    "    # Omit gatherProgenitors for first generation of halos!\n",
    "    if haloProperties_lastSnap != None: gatherProgenitors(halo)\n",
    "    setBaryonFraction(halo)\n",
    "#     fixStellarFraction(halo) # Dummy routine.\n",
    "    halo.done=True\n",
    "    \n",
    "def readProperties(halo):\n",
    "    # Reads halo properties from the input graph file\n",
    "    halo.catalogID=graphInputFile[halo.graphID][halo.snapID][halo.haloID].attrs.get('catalogID')\n",
    "    halo.mass=graphInputFile[halo.graphID][halo.snapID][halo.haloID].attrs.get('halo_mass')\n",
    "    return\n",
    "\n",
    "def calcMassToDesc(halo):\n",
    "    # Determines how much mass goes to each descendant, in proportion to desc_mass_contribution\n",
    "    desc_mass_contribution=graphInputFile[halo.graphID][halo.snapID][halo.haloID]['desc_mass_contribution']\n",
    "    desc_mass=np.array(desc_mass_contribution/np.sum(desc_mass_contribution)*halo.mass,dtype=np.float64)\n",
    "    halo.desc_mass=dict(zip(graphInputFile[halo.graphID][halo.snapID][halo.haloID]['desc_haloIDs'],desc_mass))\n",
    "\n",
    "def gatherProgenitors(halo):\n",
    "    # Collects information about material inherited from progenitors\n",
    "    # First halo in progenitor list\n",
    "    prog0_haloID=int(haloProperties_lastSnap[0].haloID)\n",
    "    for prog_haloID in graphInputFile[halo.graphID][halo.snapID][halo.haloID]['prog_haloIDs']:\n",
    "        # Position in progenitor (lastSnap) halo list\n",
    "        if debugFlag and verbosity>=3 : print('prog_haloID =',prog_haloID)\n",
    "        prog_index_lastSnap=int(prog_haloID)-prog0_haloID\n",
    "        if debugFlag and verbosity>=3: print('prog_index_lastSnap =',prog_index_lastSnap)\n",
    "        # Check halo association\n",
    "        # This is needed because HDF5 may store halos in a different order - \n",
    "        # if this happens, will need to add code to do a search over all halos\n",
    "        # in lastSnap, or to rearrange into ascending order.\n",
    "        if debugFlag:\n",
    "            assert int(haloProperties_lastSnap[prog_index_lastSnap].haloID) == int(prog_haloID)\n",
    "        # Now gather the appropriate information from the progenitor halo\n",
    "        halo.mass_fromProgenitors+=haloProperties_lastSnap[prog_index_lastSnap].desc_mass[int(halo.haloID)]\n",
    "    return\n",
    "\n",
    "def setBaryonFraction(halo):\n",
    "    halo.massBaryon=fBaryon*max(halo.mass,halo.mass_fromProgenitors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000 halos\n",
      "Processed 2000 halos\n",
      "Processed 3000 halos\n",
      "Processed 4000 halos\n",
      "Processed 5000 halos\n",
      "Processed 6000 halos\n",
      "Processed 7000 halos\n",
      "Processed 8000 halos\n",
      "Processed 9000 halos\n",
      "Processed 10000 halos\n",
      "Processed 11000 halos\n",
      "Processed 12000 halos\n",
      "Processed 13000 halos\n",
      "Processed 14000 halos\n",
      "Processed 15000 halos\n",
      "Processed 16000 halos\n",
      "Processed 17000 halos\n",
      "Processed 18000 halos\n",
      "Processed 19000 halos\n",
      "Processed 20000 halos\n",
      "Processed 21000 halos\n",
      "Processed 22000 halos\n",
      "Processed 23000 halos\n",
      "Processed 24000 halos\n",
      "Processed 25000 halos\n",
      "Processed 26000 halos\n",
      "Processed 27000 halos\n",
      "Processed 28000 halos\n",
      "Processed 29000 halos\n",
      "Processed 30000 halos\n",
      "Processed 31000 halos\n",
      "Processed 32000 halos\n",
      "Processed 33000 halos\n",
      "Processed 34000 halos\n",
      "Processed 35000 halos\n",
      "Processed 36000 halos\n",
      "Processed 37000 halos\n",
      "Processed 38000 halos\n",
      "Processed 39000 halos\n",
      "Processed 40000 halos\n",
      "Processed 41000 halos\n",
      "Processed 42000 halos\n",
      "Processed 43000 halos\n",
      "Processed 44000 halos\n",
      "Processed 45000 halos\n",
      "Processed 46000 halos\n",
      "Processed 47000 halos\n",
      "Processed 48000 halos\n",
      "Processed 49000 halos\n",
      "Processed 50000 halos\n",
      "Processed 51000 halos\n",
      "Processed 52000 halos\n",
      "Processed 53000 halos\n",
      "Processed 54000 halos\n",
      "Processed 55000 halos\n",
      "Processed 56000 halos\n",
      "Processed 57000 halos\n",
      "Processed 58000 halos\n",
      "Processed 59000 halos\n",
      "Processed 60000 halos\n",
      "Processed 61000 halos\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "closeGraphInput() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-3bfe9b43c825>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# Close input file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mcloseGraphInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraphInputFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# Close output files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: closeGraphInput() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "# Open graph input file\n",
    "# Note: can't pass undefined argument into function\n",
    "graphInputFile=openGraphInput()\n",
    "\n",
    "# Open output files\n",
    "# Note: can't pass undefined argument into function\n",
    "haloOutputFile,haloOutputData,haloOutputDataset,haloOutput_iRec=openHaloOutput()\n",
    "# Note: no attempt here to include sub-halos (galaxies).  Let's get halos right first!\n",
    "#openGalaxyOutput(galaxyOutputFile)\n",
    "\n",
    "# Iteratively loop over halos, doing whatever processing is required.\n",
    "# This assumes that halos properties depend only upon those halos in \n",
    "# their immediate past in the merger graph.\n",
    "\n",
    "# Loop over MergerGraphs.\n",
    "nHalo=0\n",
    "#nHaloGraph=0\n",
    "#nHaloSnap=0\n",
    "for graphID in graphInputFile['/']:\n",
    "    if verbosity>=2: print('Processing graph',graphID)\n",
    "    graph=graphInputFile[graphID]\n",
    "    # Loop over snapshots from first to last.\n",
    "    haloProperties_lastSnap = None\n",
    "    for snapID in graph:  # Not sure that these are guaranteed to be in increasing order\n",
    "        if verbosity>=2: print('        snapshot',snapID)\n",
    "        snap=graph[snapID]\n",
    "        # Initialise halo properties\n",
    "        haloProperties_thisSnap=[haloProperties(graphID,snapID,haloID) for haloID in snap]\n",
    "        # Loop over halos in snapshot.\n",
    "        for halo in haloProperties_thisSnap: \n",
    "            processHalo(halo)\n",
    "            nHalo +=1\n",
    "            if verbosity>=1 and nHalo%1000==0: print('Processed {:d} halos'.format(nHalo))\n",
    "        # Once all halos have been done, output results, update reference to lastSnap (and hence free memory)\n",
    "        haloOutput_iRec=outputHalos(haloProperties_thisSnap,haloOutputData,haloOutputDataset,haloOutput_iRec)\n",
    "        del haloProperties_lastSnap\n",
    "        #gc.collect() # garbage collection -- safe but very slow.\n",
    "        haloProperties_lastSnap=haloProperties_thisSnap\n",
    "        # Delete reference to this memory to free variable for new use.\n",
    "        del haloProperties_thisSnap\n",
    "        # Temporary halt to limit to finite time\n",
    "        if nHalo>=nHaloMax:\n",
    "            closeHaloOutput(haloOutputFile,haloOutputData,haloOutputDataset,haloOutput_iRec)\n",
    "            assert False\n",
    "if not debugFlag: del haloProperties_lastSnap\n",
    "\n",
    "# Close input file\n",
    "closeGraphInput(graphInputFile)\n",
    "\n",
    "# Close output files\n",
    "closeHaloOutput(haloOutputFile,haloOutputData,haloOutputDataset,haloOutput_iRec)\n",
    "#closeGalaxyOutput(galaxyOutputFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 file \"SMT13HaloOutput.hdf5\" (mode r+)>\n"
     ]
    }
   ],
   "source": [
    "print(haloOutputFile)\n",
    "haloOutputFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closeHaloOutput(haloOutputFile,haloOutputData,haloOutputDataset,haloOutput_iRec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([\n",
    "            halo.graphID,\n",
    "            halo.snapID,\n",
    "            halo.haloID,\n",
    "            halo.catalogID,\n",
    "            halo.mass,\n",
    "            halo.massBaryon,\n",
    "            halo.mass_fromProgenitors],\n",
    "        dtype=dtype_halo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closeHaloOutput(haloOutputFile,haloOutputData,haloOutputDataset,haloOutput_iRec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
